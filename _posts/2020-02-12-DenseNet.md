---
layout: post
title: DenseNet paper review
subtitle: DensetNet paper review
date:   2020-02-12 13:22:22
author: Subin Yang
categories: CNN
comments: true
---

<strong><em>Densely Connected Convolutional Networks, Gao Huang, CVPR2017</em></strong>

[Paper](https://arxiv.org/abs/1608.06993)

<br>

<h2>DenseNet 방법 요약</h2>

DenseNet은 이러한 short connection 방법을 바탕으로, 연속적으로 나오는 각각의 layer를 모두 연결하는 방법을 사용한다. 기존의 convolution layer들이 L개의 layer들에 대해서 L 번의 connection이 이루어진다. DenseNet은 L(L+1)/2 번의 direct connections이 이루어진다.

DenseNet은 여러 장점들이 있다.

- <strong>alleviate the vanishing gradient</strong>
- <strong>strengthen feature propagation</strong>
- <strong>encourage feature reuse</strong>
- <strong>substantially reduce the number of parameters</strong>

<br>

<h2>DenseNet 이전의 연구</h2>

DenseNet 이전의 연구들은 (ResNet, Highway Networks, FractalNets) covolution networks를 더 깊게, 더 정확하고 효울적으로 학습하기 위해서 input과 output에 가까운 layer들에 short connnection을 포함한 방법들을 보여주었다. 이전의 방법들의 공통점은 모두 초기 layer에서 나중의 layer로 short path들을 만들었다는 것이다.

<br>

<h2>DenseNet layer 연결의 핵심 방법</h2>

![1](https://user-images.githubusercontent.com/37301677/78472346-edee0e80-7772-11ea-8767-8061389f8658.PNG)

<br>

DenseNet은 정보의 흐름을 최대로 하는것을 보장하기 위해서, 모든 layer들을 연결한다. feed forward 특성을 유지하기 위해서, 각각의 layer들은 모든 이전 layer들로 부터 추가적인 input을 받는다. 그리고 현재의 feature map을 모든 다음의 layer들에 전달한다.

ResNet과 다르게, layer를 지나가기 전에 summation으로 feature를 합치지 않고, DenseNet은 feature들을 concatenate 한다. 따라서, k번째 layer는 모든 이전의 convoluation block의 feature map을 구성하는 k개의 input이 있고, 현재 feature map은 다음의 모든 L-k 다음 layer들에 전달된다.

이것은 L layer networks에 대해서 L(L+1)/2 connection을 이끈다. (기존의 전통적인 architecture들은 L 번의 connection이 있다.)

<br>

<h2>DenseNet layer 연결 방법의 장점</h2>

- <strong>fewer parameter, information is preserved</strong>

DenseNet은 기존의 방법들에 비해 feature map에 대한 불필요한 학습이 필요가 없기 때문에, 더 적은 수의 parameter을 요구한다.

ResNet은 identity transformation으로 정보를 보존하지만, 이러한 정보들이 학습 과정중에서 random하게 dropped 될 수 있다. 

DenseNet은 network에 더해지는 정보들을 명확하게 구분을 하면서 정보를 보존한다. DenseNet은 feature map이 그대로 보존이 되면서, feature map의 작은 집합들을 layer에 더해주면 된다.

- <strong>improve flow of information and gradient</strong>

결국엔, 각각의 layer들은 loss function과 input signal로 부터의 gradient에 direct하게 접근할 수 있어서, 더 쉽게 학습할 수 있다.

- <strong>regularizing effect, which reduces overfitting on tasks</strong>

Dense connection은 regularize 효과가 있어, overfitting을 막아주는 효과도 있다.

<br>

<h2>DenseNet model architecture</h2>

![2](https://user-images.githubusercontent.com/37301677/78472347-ef1f3b80-7772-11ea-92e0-3095e8d6883b.PNG)

<br>

DenseNet의 모델 구조는 크게 5가지로 설명이 된다.

1. <strong>Dense connectivity</strong>
2. <strong>composite function</strong>
3. <strong>pooling layers</strong>
4. <strong>growth rate</strong>
5. <strong>bottleneck layers</strong>
6. <strong>compression</strong>

ResNet은 이전의 정보를 합하는 identity function을 사용하지만, DenseNet은 여러 network를 concatenation을 사용하여 정보를 더 보존한다.

![7](https://user-images.githubusercontent.com/37301677/78472447-cea3b100-7773-11ea-8ce6-fc85057d54ba.PNG)

ResNet과 같은 연결방식은 위와 같이 residual을 더하는 방식으로 생겼다.

![8](https://user-images.githubusercontent.com/37301677/78472448-cfd4de00-7773-11ea-9460-5713f2499f87.PNG)

DesNet 수식의 function H는 위와 같이 x가 concatenation 된 함수로 구성된다.

Pooling layer들은 denseblock 사이에 downsampling 하는 역할로 사용된다.

<br>

![3](https://user-images.githubusercontent.com/37301677/78472348-f0e8ff00-7772-11ea-8ac7-4d51242c07a2.PNG)

<br>

<h2>Experiment</h2>

CIFAR, SVHN, ImageNet

Training

Classification result

- Accuracy
- Capacity
- Parameter Efficiency
- Overfitting

![4](https://user-images.githubusercontent.com/37301677/78472349-f0e8ff00-7772-11ea-9c49-43126d8853c9.PNG)

<br>

![5](https://user-images.githubusercontent.com/37301677/78472350-f1819580-7772-11ea-8ed9-cf7fa7a03392.PNG)

<br>

![6](https://user-images.githubusercontent.com/37301677/78472387-36a5c780-7773-11ea-92af-79ed6efc122d.PNG)



<br>